{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cb5f250",
   "metadata": {},
   "source": [
    "### Praktikum 6\n",
    "Lakukan percobaan penggunaan ANNOY, FAISS, dan HNSWLIB pada dataset sekunder berukuran besar (Micro Spotify) pada link berikut: https://www.kaggle.com/datasets/bwandowando/spotify-songs-with-attributes-and-lyrics/data . Download data dan load CSV filenya (pilih dataset yg pertama dari dua dataset). pilih hanya fitur numerik saja, dan lakukan normalisasi menggunakan StandardScaler. Lakukan pencarian track terdekat dan bandingkan hasilnya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb550990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memuat dataset dari Kaggle Hub...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-2544603375.py:27: DeprecationWarning: Use dataset_load() instead of load_dataset(). load_dataset() will be removed in a future version.\n",
      "  df = kagglehub.load_dataset(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Colab cache for faster access to the 'spotify-songs-with-attributes-and-lyrics' dataset.\n",
      "Gagal memuat dataset: 'spotify_songs_with_attributes_and_lyrics.csv' is not present in the dataset files. You can access the other files of the attached dataset at '/kaggle/input/spotify-songs-with-attributes-and-lyrics'\n",
      "Pastikan Anda telah melakukan autentikasi dengan akun Kaggle Anda.\n",
      "\n",
      "Melakukan preprocessing data...\n",
      "DataFrame was not loaded successfully. Cannot proceed with preprocessing and analysis.\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies if running in a new environment like Google Colab\n",
    "# !pip install kagglehub hnswlib faiss-cpu annoy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "\n",
    "# ANN Libraries\n",
    "import faiss\n",
    "from annoy import AnnoyIndex\n",
    "import hnswlib\n",
    "\n",
    "# Scikit-learn for baseline and preprocessing\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# --- 1. Load Dataset Directly from Kaggle Hub ---\n",
    "print(\"Memuat dataset dari Kaggle Hub...\")\n",
    "try:\n",
    "    # Set the path to the file you'd like to load within the dataset\n",
    "    # Corrected filename based on the error output\n",
    "    file_path = \"spotify_songs_with_attributes_and_lyrics.csv\"\n",
    "\n",
    "    # Load the latest version of the dataset\n",
    "    df = kagglehub.load_dataset(\n",
    "        KaggleDatasetAdapter.PANDAS,\n",
    "        \"bwandowando/spotify-songs-with-attributes-and-lyrics\",\n",
    "        file_path,\n",
    "    )\n",
    "    print(\"Dataset berhasil dimuat.\")\n",
    "    print(f\"Jumlah lagu dalam dataset: {len(df)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Gagal memuat dataset: {e}\")\n",
    "    print(\"Pastikan Anda telah melakukan autentikasi dengan akun Kaggle Anda.\")\n",
    "    # In Google Colab, you might need to upload your kaggle.json or authenticate.\n",
    "    # Exit is not ideal in Colab, better to just print the error and continue\n",
    "    # exit()\n",
    "\n",
    "\n",
    "# --- 2. Preprocess Data ---\n",
    "print(\"\\nMelakukan preprocessing data...\")\n",
    "# Check if df was loaded successfully before proceeding\n",
    "if 'df' in locals() and df is not None:\n",
    "    # Select numerical features for similarity search\n",
    "    features = ['danceability', 'energy', 'loudness', 'speechiness',\n",
    "                'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo']\n",
    "    X = df[features].dropna().values.astype(np.float32)\n",
    "\n",
    "    # Standardize the features for distance-based algorithms\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    print(f\"Data siap digunakan dengan {X_scaled.shape[0]} lagu dan {X_scaled.shape[1]} fitur.\")\n",
    "\n",
    "    k = 10  # Number of nearest neighbors to find\n",
    "\n",
    "    # --- 3. Exact Nearest Neighbor (Brute-force Baseline) ---\n",
    "    print(\"\\n--- Menjalankan Exact NN (Scikit-learn) ---\")\n",
    "    start_time = time.time()\n",
    "    nn = NearestNeighbors(n_neighbors=k, algorithm='brute', metric='euclidean')\n",
    "    nn.fit(X_scaled)\n",
    "    # Find neighbors for all items in the dataset\n",
    "    dist_exact, idx_exact = nn.kneighbors(X_scaled)\n",
    "    time_exact = time.time() - start_time\n",
    "    print(f\"Selesai dalam {time_exact:.3f} detik\")\n",
    "\n",
    "    # --- 4. Annoy ---\n",
    "    print(\"\\n--- Menjalankan Annoy ---\")\n",
    "    start_time = time.time()\n",
    "    f = X_scaled.shape[1]\n",
    "    index_annoy = AnnoyIndex(f, 'euclidean')\n",
    "    for i, v in enumerate(X_scaled):\n",
    "        index_annoy.add_item(i, v)\n",
    "    index_annoy.build(20) # More trees for better accuracy\n",
    "    # Find neighbors for all items by iterating\n",
    "    idx_annoy = [index_annoy.get_nns_by_vector(v, k) for v in X_scaled]\n",
    "    time_annoy = time.time() - start_time\n",
    "    print(f\"Selesai dalam {time_annoy:.3f} detik\")\n",
    "\n",
    "    # --- 5. HNSW (hnswlib) ---\n",
    "    print(\"\\n--- Menjalankan HNSW (hnswlib) ---\")\n",
    "    start_time = time.time()\n",
    "    p_hnsw = hnswlib.Index(space='l2', dim=X_scaled.shape[1])\n",
    "    p_hnsw.init_index(max_elements=X_scaled.shape[0], ef_construction=200, M=16)\n",
    "    p_hnsw.add_items(X_scaled)\n",
    "    p_hnsw.set_ef(200) # Higher ef for better accuracy\n",
    "    # Find neighbors for all items at once\n",
    "    idx_hnsw, dist_hnsw = p_hnsw.knn_query(X_scaled, k=k)\n",
    "    time_hnsw = time.time() - start_time\n",
    "    print(f\"Selesai dalam {time_hnsw:.3f} detik\")\n",
    "\n",
    "    # --- 6. FAISS (IVFFlat) ---\n",
    "    print(\"\\n--- Menjalankan FAISS (IVFFlat) ---\")\n",
    "    start_time = time.time()\n",
    "    quantizer = faiss.IndexFlatL2(X_scaled.shape[1])\n",
    "    nlist = 100 # Number of clusters\n",
    "    index_faiss = faiss.IndexIVFFlat(quantizer, X_scaled.shape[1], nlist, faiss.METRIC_L2)\n",
    "    index_faiss.train(X_scaled)\n",
    "    index_faiss.add(X_scaled)\n",
    "    index_faiss.nprobe = 10 # Search in 10 nearest clusters\n",
    "    # Find neighbors for all items at once\n",
    "    dist_faiss, idx_faiss = index_faiss.search(X_scaled, k)\n",
    "    time_faiss = time.time() - start_time\n",
    "    print(f\"Selesai dalam {time_faiss:.3f} detik\")\n",
    "\n",
    "    # --- 7. Final Results Comparison ---\n",
    "    print(\"\\n==============================================\")\n",
    "    print(\"              Ringkasan Waktu Proses\")\n",
    "    print(\"==============================================\")\n",
    "    print(f\"Exact NN (Brute-force): {time_exact:>7.3f} detik\")\n",
    "    print(f\"Annoy                 : {time_annoy:>7.3f} detik\")\n",
    "    print(f\"HNSW (hnswlib)        : {time_hnsw:>7.3f} detik\")\n",
    "    print(f\"FAISS (IVFFlat)       : {time_faiss:>7.3f} detik\")\n",
    "    print(\"==============================================\")\n",
    "\n",
    "\n",
    "    print(\"\\nContoh perbandingan top-5 tetangga terdekat untuk lagu pertama:\")\n",
    "    # Ensure df is not empty before accessing iloc\n",
    "    if not df.empty:\n",
    "        first_song_title = df['track_name'].iloc[0]\n",
    "        print(f\"Lagu: '{first_song_title}'\")\n",
    "        print(f\"Exact NN: {idx_exact[0][:5]}\")\n",
    "        print(f\"Annoy:    {idx_annoy[0][:5]}\")\n",
    "        print(f\"HNSW:     {idx_hnsw[0][:5]}\")\n",
    "        print(f\"FAISS:    {idx_faiss[0][:5]}\")\n",
    "    else:\n",
    "        print(\"DataFrame is empty, cannot display song title and neighbors.\")\n",
    "\n",
    "else:\n",
    "    print(\"DataFrame was not loaded successfully. Cannot proceed with preprocessing and analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a405ae",
   "metadata": {},
   "source": [
    "Tugas\n",
    "Jalankan code berikut pada Google Colab dan PyDroid3 (Android Python) Application di Smartphone Android. Bandingkan hasilnya dan tuliskan analisa anda, tuliskan juga spesifikasi smartphone yang anda gunakan :)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6c5e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from annoy import AnnoyIndex\n",
    "import hnswlib\n",
    "import faiss\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# -------------------------------\n",
    "# Contoh dataset kecil untuk testing\n",
    "# -------------------------------\n",
    "np.random.seed(42)\n",
    "n_samples = 10000   # jumlah database vector\n",
    "d = 128             # dimensi\n",
    "X = np.random.random((n_samples, d)).astype('float32')\n",
    "\n",
    "# Standarisasi fitur\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "k = 10  # jumlah nearest neighbors\n",
    "\n",
    "# -------------------------------\n",
    "# Exact NN (brute-force)\n",
    "# -------------------------------\n",
    "start = time.time()\n",
    "nn = NearestNeighbors(n_neighbors=k, algorithm='brute', metric='euclidean')\n",
    "nn.fit(X_scaled)\n",
    "dist_exact, idx_exact = nn.kneighbors(X_scaled)\n",
    "time_exact = time.time() - start\n",
    "print(f\"Exact NN done in {time_exact:.3f} s\")\n",
    "\n",
    "# -------------------------------\n",
    "# Annoy\n",
    "# -------------------------------\n",
    "start = time.time()\n",
    "f = X_scaled.shape[1]\n",
    "index_annoy = AnnoyIndex(f, 'euclidean')\n",
    "for i, v in enumerate(X_scaled):\n",
    "    index_annoy.add_item(i, v)\n",
    "index_annoy.build(10)\n",
    "idx_annoy = [index_annoy.get_nns_by_vector(v, k) for v in X_scaled]\n",
    "time_annoy = time.time() - start\n",
    "print(f\"Annoy done in {time_annoy:.3f} s\")\n",
    "\n",
    "# -------------------------------\n",
    "# HNSW\n",
    "# -------------------------------\n",
    "start = time.time()\n",
    "p_hnsw = hnswlib.Index(space='l2', dim=d)\n",
    "p_hnsw.init_index(max_elements=n_samples, ef_construction=200, M=16)\n",
    "p_hnsw.add_items(X_scaled)\n",
    "p_hnsw.set_ef(200)\n",
    "idx_hnsw, _ = p_hnsw.knn_query(X_scaled, k=k)\n",
    "time_hnsw = time.time() - start\n",
    "print(f\"HNSW done in {time_hnsw:.3f} s\")\n",
    "\n",
    "# -------------------------------\n",
    "# FAISS IVF\n",
    "# -------------------------------\n",
    "start = time.time()\n",
    "quantizer = faiss.IndexFlatL2(d)\n",
    "index_faiss = faiss.IndexIVFFlat(quantizer, d, nlist=100, metric=faiss.METRIC_L2)\n",
    "index_faiss.train(X_scaled)\n",
    "index_faiss.add(X_scaled)\n",
    "index_faiss.nprobe = 10\n",
    "_, idx_faiss = index_faiss.search(X_scaled, k)\n",
    "time_faiss = time.time() - start\n",
    "print(f\"FAISS IVF done in {time_faiss:.3f} s\")\n",
    "\n",
    "# -------------------------------\n",
    "# Tampilkan ringkasan waktu\n",
    "# -------------------------------\n",
    "print(\"\\n=== Ringkasan Waktu (detik) ===\")\n",
    "print(f\"Exact NN : {time_exact:.3f}\")\n",
    "print(f\"Annoy    : {time_annoy:.3f}\")\n",
    "print(f\"HNSW     : {time_hnsw:.3f}\")\n",
    "print(f\"FAISS    : {time_faiss:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
